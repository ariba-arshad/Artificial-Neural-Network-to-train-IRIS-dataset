{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J5LmtfBMGrIh"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "#Data set\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V87_TxoUfHZL"
   },
   "outputs": [],
   "source": [
    "#Forward propagation\n",
    "from decimal import Decimal, getcontext\n",
    "import math\n",
    "def forward_prop():\n",
    "    global X,h,O,w_hidden,w_output,b_hidden,b_output\n",
    "\n",
    "    #calculate hidden layer values\n",
    "    for i in range(5):\n",
    "        val=0.0\n",
    "        for j in range(4):\n",
    "            val=val+(X[j]*w_hidden[j][i])\n",
    "        #using relu on hidden layer f(x)=(0,x)\n",
    "        h[i]=max(0,val+b_hidden[i])\n",
    "\n",
    "\n",
    "    #calculate output layer values\n",
    "    for i in range(3):\n",
    "        val=0.0\n",
    "        for j in range(5):\n",
    "            val=val+(h[j]*w_output[j][i])\n",
    "        O[i]=val+b_output[i]\n",
    "\n",
    "    # Set the precision for Decimal calculations\n",
    "    getcontext().prec = 30\n",
    "\n",
    "    max_O = max(O)\n",
    "    sum_decimal = Decimal('0.0')\n",
    "    exp_decimal = []\n",
    "\n",
    "    # Compute the exponential of each value and sum them\n",
    "    for i in range(3):\n",
    "        exp_decimal.append(Decimal(math.exp(O[i]- max_O)))\n",
    "        sum_decimal += exp_decimal[i]\n",
    "\n",
    "    for i in range(3):\n",
    "        O[i] = exp_decimal[i] / sum_decimal\n",
    "\n",
    "    # Convert Decimal values to floats\n",
    "    O = [float(x) for x in O]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "oTT1gOBcfJo5"
   },
   "outputs": [],
   "source": [
    "#For Error calculation\n",
    "def abs_error():\n",
    "  global E,actual_output,O\n",
    "  for i in range(3):\n",
    "      E[i]=abs(actual_output[i]-O[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dxA3MG_ifLzK"
   },
   "outputs": [],
   "source": [
    "#-----------------Back propagation------------------------\n",
    "#calculating delta_O,delta_h\n",
    "def func1():\n",
    "    global delta_O,delta_h,E,O,h,w_output\n",
    "\n",
    "    for i in range(len(delta_O)):\n",
    "          #derivative of softmax is x(1-x)\n",
    "          delta_O[i]=E[i] * (O[i]*(1-O[i]))\n",
    "\n",
    "    for i in range(len(h)):\n",
    "        val=0.0\n",
    "        for j in range(len(delta_O)):\n",
    "            #using derivative of relu i.e if x>0 then 1 else 0\n",
    "            if h[i]>0:\n",
    "                val+=delta_O[j]*(1)*w_output[i][j]\n",
    "            else:\n",
    "                val+=delta_O[j]*(0)*w_output[i][j]\n",
    "        delta_h[i]=val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rb67FXAkfNpd"
   },
   "outputs": [],
   "source": [
    "#Update weights\n",
    "\n",
    "def update_weights():\n",
    "    global learning_rate,w_hidden,w_output,delta_h,X,h,delta_O\n",
    "    #update hidden layer weights\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            w_hidden[i][j]+=(learning_rate*delta_h[j]*X[i])\n",
    "\n",
    "\n",
    "    #update output layer weights\n",
    "    for i in range(5):\n",
    "        for j in range(3):\n",
    "            w_output[i][j]+=(learning_rate*delta_O[j]*h[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lbce7UZffPPn"
   },
   "outputs": [],
   "source": [
    "#Update biases\n",
    "\n",
    "def update_bias():\n",
    "    global b_hidden,b_output,learning_rate,delta_h,delta_O\n",
    "\n",
    "    #update hidden layer bias\n",
    "    for i in range(len(b_hidden)):\n",
    "        b_hidden[i]+=(learning_rate*delta_h[i])\n",
    "\n",
    "    #update output layer bias\n",
    "    for i in range(len(b_output)):\n",
    "        b_output[i]+=(learning_rate*delta_O[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tkqVbpoNfQ1M"
   },
   "outputs": [],
   "source": [
    "def backward_prop():\n",
    "    func1()\n",
    "    update_weights()\n",
    "    update_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbsMlob5lZt_",
    "outputId": "d36f993f-04c7-4420-8f55-051ac90b573e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29  out of  30  correct.\n",
      "Accuracy= 96.67 %\n",
      "loss=\n",
      "0.008\n",
      "0.954\n",
      "0.962\n"
     ]
    }
   ],
   "source": [
    "#***********************************************************\n",
    "def train_MLP(X_train, y_train):\n",
    "    global X, actual_output, E, delta_O, delta_h\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        # Set input layer to current sample\n",
    "        X = X_train[i]\n",
    "        # Set actual output based on the label of the current sample\n",
    "        if y_train[i] == 'Iris-setosa':\n",
    "            actual_output = [1, 0, 0]\n",
    "        elif y_train[i] == 'Iris-versicolor':\n",
    "            actual_output = [0, 1, 0]\n",
    "        elif y_train[i] == 'Iris-virginica':\n",
    "            actual_output = [0, 0, 1]\n",
    "\n",
    "        # Forward propagation\n",
    "        forward_prop()\n",
    "\n",
    "        # Calculate absolute error\n",
    "        abs_error()\n",
    "\n",
    "        # Backward propagation\n",
    "        backward_prop()\n",
    "\n",
    "#***********************************************************\n",
    "def test_MLP(X_test,y_test):\n",
    "    global X,E\n",
    "    sum_loss=[0,0,0]\n",
    "    correct_predictions=0\n",
    "    for i in range(len(X_test)):\n",
    "        # Set input layer to current sample\n",
    "        X = X_test[i]\n",
    "\n",
    "        # Forward propagation\n",
    "        forward_prop()\n",
    "\n",
    "        abs_error()\n",
    "        #add error\n",
    "        for i in range(len(sum_loss)):\n",
    "          sum_loss[i]+=E[i]\n",
    "\n",
    "        #Check result\n",
    "        if (y_train[i] == 'Iris-setosa' and O[0]==max(O)) or (y_train[i] == 'Iris-versicolor' and O[1]==max(O)) or (y_train[i] == 'Iris-virginica' and O[2]==max(O)):\n",
    "            correct_predictions+=1\n",
    "    return correct_predictions,sum_loss\n",
    "\n",
    "\n",
    "#***********************************************************\n",
    "df=pd.read_csv('Iris.csv')\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]  # Features\n",
    "y = df['Species']  # Label\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()\n",
    "\n",
    "\n",
    "#***********************************************************\n",
    "import random\n",
    "#---------------------------------------\n",
    "#Setting input layer\n",
    "X=[1,1,1,1]  #since IRIS has 4 input columns\n",
    "\n",
    "#Setting hidden layer\n",
    "h=[0,0,0,0,0]\n",
    "\n",
    "#Setting output layer\n",
    "O=[0,0,0]  #since IRIS has 3 classes O[0]=Setosa,O[1]=Versicolour,O[2]=Virginica\n",
    "\n",
    "#Setting weights between input and hidden layer\n",
    "\n",
    "w_hidden=[]\n",
    "for i in range(4):\n",
    "    row=[]\n",
    "    for j in range(5):\n",
    "        row.append(random.uniform(0,1))\n",
    "    w_hidden.append(row)\n",
    "\n",
    "\n",
    "#Setting weights between hidden and output layer\n",
    "\n",
    "w_output=[]\n",
    "\n",
    "for i in range(5):\n",
    "    row=[]\n",
    "    for j in range(3):\n",
    "        row.append(random.uniform(0,1))\n",
    "    w_output.append(row)\n",
    "\n",
    "#Setting hidden layer bias\n",
    "b_hidden=[]\n",
    "for i in range(5):\n",
    "    b_hidden.append(random.uniform(0,1))\n",
    "\n",
    "#Setting output layer bias\n",
    "b_output=[]\n",
    "for i in range(3):\n",
    "    b_output.append(random.uniform(0,1))\n",
    "\n",
    "# One-hot encoded label\n",
    "actual_output = [0, 0, 0]\n",
    "\n",
    "#Absolute error\n",
    "E=[0,0,0]\n",
    "\n",
    "delta_O=[0,0,0]\n",
    "delta_h=[0,0,0,0,0]\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "#****************************************************************\n",
    "\n",
    "# Training loop\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    train_MLP(X_train, y_train)\n",
    "\n",
    "#***********************************************************\n",
    "correct_predictions,sum_loss=test_MLP(X_test,y_test)\n",
    "print(correct_predictions,' out of ',len(X_test),' correct.')\n",
    "accuracy=correct_predictions/len(X_test)\n",
    "print('Accuracy=',round(accuracy*100,2),'%')\n",
    "print('loss=')\n",
    "for i in range(len(sum_loss)):\n",
    "  print(round(sum_loss[i]/len(X_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ie-cZLua2ril",
    "outputId": "e1034e3e-7be8-4583-ee49-02e4adad26c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26  out of  30  correct.\n",
      "Accuracy= 86.67 %\n",
      "loss=\n",
      "0.146\n",
      "0.854\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#***************Using different range for weights********************************************\n",
    "#Using IRIS dataset\n",
    "def train_MLP(X_train, y_train):\n",
    "    global X\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        # Set input layer to current sample\n",
    "        X = X_train[i]\n",
    "        # Set actual output based on the label of the current sample\n",
    "        if y_train[i] == 'Iris-setosa':\n",
    "            actual_output = [1, 0, 0]\n",
    "        elif y_train[i] == 'Iris-versicolor':\n",
    "            actual_output = [0, 1, 0]\n",
    "        elif y_train[i] == 'Iris-virginica':\n",
    "            actual_output = [0, 0, 1]\n",
    "\n",
    "        # Forward propagation\n",
    "        forward_prop()\n",
    "\n",
    "        # Calculate absolute error\n",
    "        abs_error()\n",
    "\n",
    "        # Backward propagation\n",
    "        backward_prop()\n",
    "\n",
    "#***********************************************************\n",
    "def test_MLP(X_test,y_test):\n",
    "    global X,E\n",
    "    sum_loss=[0,0,0]\n",
    "    correct_predictions=0\n",
    "    for i in range(len(X_test)):\n",
    "        # Set input layer to current sample\n",
    "        X = X_test[i]\n",
    "\n",
    "        # Forward propagation\n",
    "        forward_prop()\n",
    "\n",
    "        abs_error()\n",
    "        #add error\n",
    "        for i in range(len(sum_loss)):\n",
    "          sum_loss[i]+=E[i]\n",
    "\n",
    "        #Check result\n",
    "        if (y_train[i] == 'Iris-setosa' and O[0]==max(O)) or (y_train[i] == 'Iris-versicolor' and O[1]==max(O)) or (y_train[i] == 'Iris-virginica' and O[2]==max(O)):\n",
    "            correct_predictions+=1\n",
    "    return correct_predictions,sum_loss\n",
    "\n",
    "\n",
    "#***********************************************************\n",
    "df=pd.read_csv('Iris.csv')\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]  # Features\n",
    "y = df['Species']  # Label\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()\n",
    "\n",
    "\n",
    "#***********************************************************\n",
    "import random\n",
    "#---------------------------------------\n",
    "#Setting input layer\n",
    "X=[1,1,1,1]  #since IRIS has 4 input columns\n",
    "\n",
    "#Setting hidden layer\n",
    "h=[0,0,0,0,0]\n",
    "\n",
    "#Setting output layer\n",
    "O=[0,0,0]  #since IRIS has 3 classes O[0]=Setosa,O[1]=Versicolour,O[2]=Virginica\n",
    "\n",
    "#Setting weights between input and hidden layer\n",
    "\n",
    "w_hidden=[]\n",
    "for i in range(4):\n",
    "    row=[]\n",
    "    for j in range(5):\n",
    "        row.append(random.uniform(-1,1))\n",
    "    w_hidden.append(row)\n",
    "\n",
    "\n",
    "#Setting weights between hidden and output layer\n",
    "\n",
    "w_output=[]\n",
    "\n",
    "for i in range(5):\n",
    "    row=[]\n",
    "    for j in range(3):\n",
    "        row.append(random.uniform(-1,1))\n",
    "    w_output.append(row)\n",
    "\n",
    "#Setting hidden layer bias\n",
    "b_hidden=[]\n",
    "for i in range(5):\n",
    "    b_hidden.append(random.uniform(-1,1))\n",
    "\n",
    "#Setting output layer bias\n",
    "b_output=[]\n",
    "for i in range(3):\n",
    "    b_output.append(random.uniform(-1,1))\n",
    "\n",
    "# One-hot encoded label\n",
    "actual_output = [0, 0, 0]\n",
    "\n",
    "#Absolute error\n",
    "loss = 0.0\n",
    "\n",
    "E=[0,0,0]\n",
    "\n",
    "delta_O=[0,0,0]\n",
    "delta_h=[0,0,0,0,0]\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "#****************************************************************\n",
    "\n",
    "# Training loop\n",
    "iterations = 1000\n",
    "for i in range(iterations):\n",
    "    train_MLP(X_train, y_train)\n",
    "\n",
    "#***********************************************************\n",
    "correct_predictions,sum_loss=test_MLP(X_test,y_test)\n",
    "print(correct_predictions,' out of ',len(X_test),' correct.')\n",
    "accuracy=correct_predictions/len(X_test)\n",
    "print('Accuracy=',round(accuracy*100,2),'%')\n",
    "print('loss=')\n",
    "for i in range(len(sum_loss)):\n",
    "  print(round(sum_loss[i]/len(X_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYXTsnr84Qhz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
